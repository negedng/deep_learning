{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with pretrained CNN\n",
    "### Using features from https://github.com/fchollet/deep-learning-models\n",
    "    VGG16, VGG19, ResNet50, Inception v3 and CRNN for music tagging in Keras under MIT license\n",
    "    \n",
    "## Import\n",
    "* Note: keras.metrics' fmeasure, precision and recall functions are newer than the last keras update, you may need to reload it manually(2016.11.05)\n",
    "\n",
    ">  wget https://raw.githubusercontent.com/fchollet/keras/master/keras/metrics.py\n",
    "\n",
    ">  sudo cp metrics.py /usr/local/lib/python2.7/dist-packages/keras/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.metrics import binary_accuracy, fmeasure, precision, recall\n",
    "from keras.optimizers import SGD,adam\n",
    "\n",
    "from models.imagenet_utils import decode_predictions, preprocess_input\n",
    "from models.vgg16 import VGG16\n",
    "\n",
    "import urllib\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load VGG16\n",
    "\n",
    "#### VGG16 conception: last convolutional block and fully-connected trainable\n",
    "Note: The loaded model use 3 dense layers in the last block\n",
    "<img src=\"assets/vgg16.png\" width=\"350\">\n",
    "<em>image: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html<em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K.image_dim_ordering: th\n"
     ]
    }
   ],
   "source": [
    "model = VGG16(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess images (ft. from GOID)\n",
    "* Google Open Image Dataset: https://github.com/openimages/dataset\n",
    "* [Preprocess Dataset](collect_data_info.py) URL-label pairs for the labels given in [this file](assets/dict.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_URL = []\n",
    "with open(\"assets/train_url_labels.csv\") as f:\n",
    "    for line in f:\n",
    "        next_line = line.split(\",\")\n",
    "        train_data_URL.append([next_line[0],next_line[1].rstrip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8093"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating (1:1000) np array from label (imagenet 1000 labels set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgnet_dict = {}\n",
    "with open('assets/dict.csv') as f:\n",
    "    for line in f:\n",
    "        next_line = line.split('\\t')\n",
    "        imgnet_dict[next_line[1]]=int(next_line[2].rstrip())\n",
    "\n",
    "#Better solution if we store the probability from the GOID and feed with that instead of 1.0\n",
    "def generate_Y(label):\n",
    "    a = np.zeros(1000, dtype=\"float32\")\n",
    "    a[imgnet_dict[label]]=1.0\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training data\n",
    "This may take several minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "layers_num = {}\n",
    "_end = len(train_data_URL)\n",
    "for i in range(0,_end):\n",
    "    if(layers_num.get(train_data_URL[i][1])!=None):\n",
    "        layers_num[train_data_URL[i][1]]+=1\n",
    "    else:\n",
    "        layers_num[train_data_URL[i][1]]=1\n",
    "    num = layers_num[train_data_URL[i][1]]\n",
    "    if num<=30 :\n",
    "        try:\n",
    "            img_path = urllib.request.urlopen(train_data_URL[i][0])\n",
    "            img = image.load_img(img_path, target_size=(224,224))\n",
    "            x = image.img_to_array(img)\n",
    "            x = np.expand_dims(x, axis=0)\n",
    "            x = preprocess_input(x)\n",
    "            train_data.append([x,train_data_URL[i][1]])\n",
    "            if(np.floor(i/_end*100)>np.floor((i-1)/_end*100)):\n",
    "                sys.stdout.write(\">\")\n",
    "        except ValueError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_data_URL = []\n",
    "with open(\"assets/validation_url_labels.csv\") as f:\n",
    "    for line in f:\n",
    "        next_line = line.split(\",\")\n",
    "        validation_data_URL.append([next_line[0],next_line[1].rstrip()])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>>"
     ]
    }
   ],
   "source": [
    "validation_data = []\n",
    "layers_num = {}\n",
    "_end = len(validation_data_URL)\n",
    "for i in range(0,_end):\n",
    "    if(layers_num.get(validation_data_URL[i][1])!=None):\n",
    "        layers_num[validation_data_URL[i][1]]+=1\n",
    "    else:\n",
    "        layers_num[validation_data_URL[i][1]]=1\n",
    "    num = layers_num[validation_data_URL[i][1]]\n",
    "    if num<=20 :\n",
    "        try:\n",
    "            img_path = urllib.request.urlopen(validation_data_URL[i][0])\n",
    "            img = image.load_img(img_path, target_size=(224,224))\n",
    "            x = image.img_to_array(img)\n",
    "            x = np.expand_dims(x, axis=0)\n",
    "            x = preprocess_input(x)\n",
    "            validation_data.append([x,validation_data_URL[i][1]])\n",
    "            if(np.floor(i/_end*100)>np.floor((i-1)/_end*100)):\n",
    "                sys.stdout.write(\">\")\n",
    "        except ValueError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cut test data from validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_test_rate = 0.75\n",
    "test_data = validation_data[round(valid_test_rate*len(validation_data)):]\n",
    "validation_data = validation_data[:round(valid_test_rate*len(validation_data))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training-validation-test: 30-15-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Only training fully connected first, later on train all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.topology.InputLayer object at 0x7fe2ccdfb8d0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe2ccdfba20>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29dff7dd8>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fe29dfd3710>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29dfcfb00>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29df836d8>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fe29df97550>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29df97748>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29df25c50>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29df2e978>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fe29df48b70>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29df48d68>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29df4eba8>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29dee6d30>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fe29deeeb70>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29deeed68>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29df05a90>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29df17f98>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fe29dea6b00>\n",
      "\n",
      "<keras.layers.core.Flatten object at 0x7fe29dea6cf8>\n",
      "<keras.layers.core.Dense object at 0x7fe29dde48d0>\n",
      "<keras.layers.core.Dense object at 0x7fe29ded19e8>\n",
      "<keras.layers.core.Dense object at 0x7fe29dda6400>\n"
     ]
    }
   ],
   "source": [
    "#freeze layers\n",
    "for layer in model.layers[:19]:\n",
    "    layer.trainable = False\n",
    "    print(layer)\n",
    "print(\"\")\n",
    "for layer in model.layers[19:]:\n",
    "    layer.trainable = True\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt = adam(lr=0.5)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt,\n",
    "metrics=['binary_accuracy', 'fmeasure', 'precision', 'recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving input shape missmatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_sets_tmp = []\n",
    "tr_vals_tmp = []\n",
    "for line in train_data:\n",
    "    tr_sets_tmp.append(line[0][0])\n",
    "    tr_vals_tmp.append(generate_Y(line[1]))\n",
    "val_sets_tmp = []\n",
    "val_vals_tmp = []\n",
    "for line in validation_data:\n",
    "    val_sets_tmp.append(line[0][0])\n",
    "    val_vals_tmp.append(generate_Y(line[1]))\n",
    "test_sets = []\n",
    "test_vals = []\n",
    "for line in test_data:\n",
    "    test_sets.append(line[0][0])\n",
    "    test_vals.append(generate_Y(line[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_sets = np.array(tr_sets_tmp,  dtype= \"float32\")\n",
    "tr_vals = np.array(tr_vals_tmp)\n",
    "val_sets = np.array(val_sets_tmp,  dtype= \"float32\")\n",
    "val_vals = np.array(val_vals_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(266, 3, 224, 224)\n",
      "(266, 1000)\n",
      "(141, 3, 224, 224)\n",
      "(141, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(tr_sets))\n",
    "print(np.shape(tr_vals))\n",
    "print(np.shape(val_sets))\n",
    "print(np.shape(val_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 266 samples, validate on 141 samples\n",
      "Epoch 1/20\n",
      "266/266 [==============================] - 118s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 2/20\n",
      "266/266 [==============================] - 118s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 3/20\n",
      "266/266 [==============================] - 118s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 4/20\n",
      "266/266 [==============================] - 121s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 5/20\n",
      "266/266 [==============================] - 130s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 6/20\n",
      "266/266 [==============================] - 134s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 7/20\n",
      "266/266 [==============================] - 133s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 8/20\n",
      "266/266 [==============================] - 132s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 9/20\n",
      "266/266 [==============================] - 136s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 10/20\n",
      "266/266 [==============================] - 136s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 11/20\n",
      "266/266 [==============================] - 137s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 12/20\n",
      "266/266 [==============================] - 135s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 13/20\n",
      "266/266 [==============================] - 136s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 14/20\n",
      "266/266 [==============================] - 135s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 15/20\n",
      "266/266 [==============================] - 134s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 16/20\n",
      "266/266 [==============================] - 132s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 17/20\n",
      "266/266 [==============================] - 134s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 18/20\n",
      "266/266 [==============================] - 134s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 19/20\n",
      "266/266 [==============================] - 134s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 20/20\n",
      "266/266 [==============================] - 134s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe299130908>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tr_sets,tr_vals,\n",
    "          nb_epoch=20,\n",
    "          verbose=1,\n",
    "          batch_size = 32,\n",
    "          validation_data=(val_sets, val_vals),\n",
    "         shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.topology.InputLayer object at 0x7fe2ccdfb8d0>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe2ccdfba20>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29dff7dd8>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fe29dfd3710>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29dfcfb00>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29df836d8>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fe29df97550>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29df97748>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29df25c50>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29df2e978>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fe29df48b70>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29df48d68>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29df4eba8>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29dee6d30>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fe29deeeb70>\n",
      "\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29deeed68>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29df05a90>\n",
      "<keras.layers.convolutional.Convolution2D object at 0x7fe29df17f98>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fe29dea6b00>\n",
      "<keras.layers.core.Flatten object at 0x7fe29dea6cf8>\n",
      "<keras.layers.core.Dense object at 0x7fe29dde48d0>\n",
      "<keras.layers.core.Dense object at 0x7fe29ded19e8>\n",
      "<keras.layers.core.Dense object at 0x7fe29dda6400>\n"
     ]
    }
   ],
   "source": [
    "#freeze layers\n",
    "for layer in model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "    print(layer)\n",
    "print(\"\")\n",
    "for layer in model.layers[15:]:\n",
    "    layer.trainable = True\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 266 samples, validate on 141 samples\n",
      "Epoch 1/20\n",
      "266/266 [==============================] - 135s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 2/20\n",
      "266/266 [==============================] - 135s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 3/20\n",
      "266/266 [==============================] - 133s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 4/20\n",
      "266/266 [==============================] - 134s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 5/20\n",
      "266/266 [==============================] - 130s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 6/20\n",
      "266/266 [==============================] - 134s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 7/20\n",
      "266/266 [==============================] - 136s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 8/20\n",
      "266/266 [==============================] - 132s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 9/20\n",
      "266/266 [==============================] - 136s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 10/20\n",
      "266/266 [==============================] - 136s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 11/20\n",
      "266/266 [==============================] - 137s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 12/20\n",
      "266/266 [==============================] - 137s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 13/20\n",
      "266/266 [==============================] - 136s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 14/20\n",
      "266/266 [==============================] - 135s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 15/20\n",
      "266/266 [==============================] - 137s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 16/20\n",
      "266/266 [==============================] - 138s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 17/20\n",
      "266/266 [==============================] - 136s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 18/20\n",
      "266/266 [==============================] - 135s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 19/20\n",
      "266/266 [==============================] - 140s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n",
      "Epoch 20/20\n",
      "266/266 [==============================] - 136s - loss: 14.4820 - binary_accuracy: 0.9982 - fmeasure: 0.1015 - precision: 0.1015 - recall: 0.1015 - val_loss: 14.6320 - val_binary_accuracy: 0.9982 - val_fmeasure: 0.0922 - val_precision: 0.0922 - val_recall: 0.0922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe277f2c2e8>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tr_sets,tr_vals,\n",
    "          nb_epoch=20,\n",
    "          verbose=1,\n",
    "          batch_size = 32,\n",
    "          validation_data=(val_sets, val_vals),\n",
    "         shuffle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 224, 224)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([val_sets[27]])\n",
    "preds = model.predict(a)\n",
    "np.shape(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formating the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n01981276', 'king_crab', 1.0), ('n15075141', 'toilet_tissue', 0.0), ('n02319095', 'sea_urchin', 0.0), ('n02395406', 'hog', 0.0), ('n02391049', 'zebra', 0.0)], [('n01443537', 'goldfish', 1.0), ('n15075141', 'toilet_tissue', 0.0), ('n02319095', 'sea_urchin', 0.0), ('n02395406', 'hog', 0.0), ('n02391049', 'zebra', 0.0)]]\n"
     ]
    }
   ],
   "source": [
    "print('Predicted:', decode_predictions(np.array([val_vals[0],val_vals[1]])))\n",
    "# print: [[u'n02504458', u'African_elephant']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1000)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape([val_vals[0],val_vals[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
